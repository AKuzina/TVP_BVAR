\subsection{TVP-BVAR}
\subsubsection{Модель}
Наконец, рассмотрим байесовскую векторную авторегрессию с переменными коэффициентами и стохастической волатильностью, описанную в статье Primiceri (2005). Как видно из названия, она представляет из себя векторную авторегрессию, в которой коэффициенты и ковариационная матрица остатков изменяются со временем и оценки неизвестных параметров находятся посредством байесовского подхода. Построение такого рода моделей позволяет учитывать нелинейности зависимостей между разными переменными или их временные изменения.

Обозначим вектор из $n$ эндогенных переменных как $y_t$  и рассмотрим основное уравнение модели:
\begin{equation}\label{tvp1}
y_t = c_t + B_{1,t}y_{t-1} + \cdots + B_{k,t}y_{t-k} + \varepsilon_t, \quad t = 1,...,T,
\end{equation}
где $c_t$ --- $n \times 1$ вектор свободных коэффициентов, меняющийся во времени, $B_{i,t}, i = 1,...,p$ --- $n \times n$ матрицы коэффициентов, также зависящих от момента времени, $u_t$ --- гетероскедастичные остатки с ковариационной матрицей $\Omega_t$. Авторы приводят ковариационную матрицу остатков к диагональному виду, используя следующее преобразование:
\begin{equation}\label{error_decomp}
A_t \Omega_t A'_t = \Sigma_t\Sigma'_t
\end{equation}

Здесь матрица $A_t$ является нижнетреугольной и содержит в себе параметры, отвечающие за связь между ортогонализированными остатками модели, а $\Sigma_t$ является диагональной и состоит из стандартных ошибок оргонализированных остатков:\\
\begin{align*}
A_t = 
  \begin{bmatrix}
  1             & 0          & \dots          & 0\\
  \alpha_{21,t} & 1          &\dots           & \vdots\\
  \vdots        &\vdots      & \ddots         & 0\\
  \alpha_{n1,t} & \dots      &\alpha_{nn-1,t} & 1
  \end{bmatrix}
%\end{equation*}
\quad
%\begin{equation*}
\Sigma_t = 
  \begin{bmatrix}
  \sigma_{1,t}  & 0          & \dots          & 0\\
  0             & \sigma_{2,t}&\dots           & \vdots\\
  \vdots        &\vdots      & \ddots         & 0\\
  0             & \dots      &0                & \sigma_{n,t}
  \end{bmatrix}
\end{align*}
При таком разложении шоки модели можно выразить как:
\begin{equation*}
u_t = A^{-1}\Sigma_t\varepsilon_t, \text{где} \medspace {\rm Var}(\varepsilon_t) = I_n
\end{equation*}
Не сложно показать, что дисперсия шоков осталась прежней:\\
\begin{equation*}
{\rm Var}(u_t) = A^{-1}\Sigma_t{\rm Var}(\varepsilon_t)(A^{-1}\Sigma_t)' = A^{-1}\Sigma_t\Sigma'_t(A^{-1}_t)' = \Omega_t
\end{equation*}

В итоге, мы можем рассмотреть модель пространства состояний, содержащую уравнение измерений \eqref{tvp1} и три уравнения переходов: для параметров $B_t$ и значений матрицы $A_t$ (не равных 0 или 1), а также для диагональных элементов матрицы $\Omega_t$. Пусть $\alpha_t$ --- вектор всех переменных элементов матрицы $A_t$, а $\sigma_t$ --- вектор диагональных элементов $\Omega_t$. Тогда полная модель имеет следующий вид:
\begin{align}
y_t &= X'_tB_t + A_t^{-1}\Omega_t\varepsilon_t \label{tvp2}\\
B_t &= B_{t-1} + \nu_t, &\text{где} \medspace \nu_t \sim N(0, Q)\label{beta}\\
\alpha_t &= \alpha_{t-1} + \zeta_t, &\text{где} \medspace\zeta_t \sim N(0, S)\label{alpha}\\
\log \sigma_t &= \log \sigma_{t-1} + \eta_t, &\text{где} \medspace \eta_t \sim N(0, W)\label{sigma}
\end{align}

Отметим, что уравнение \eqref{tvp2} отличается от \eqref{tvp1} только тем, что оно записано в матричной форме, из чего следует что $X'_t = I_n \otimes \lbrack 1, y'_{t-1}, \dots, y'_{t-k}\rbrack$ --- матрица размера $n \times n(k+1)$, а $B_t$ --- вектор размерности $n(k+1) \times 1$, состоящий из всех коэффициентов $\{B_{i,t}\}_{i=1}^{k}$ и свободных коэффициентов $c_t$ в момент времени $t$. 

Автор модели предполагает, что элементы вектора $B_t$ и переменные коэффициенты матрицы $A_t$ изменяются по закону случайного блуждания, в то время как стандартные отклонения $\sigma_t$, входящие в модель мультипликативно, моделируются с помощью процесса геометрического случайного блуждания.

Наконец, предполагается, что случайные величины $\{\varepsilon_t, \nu_t, \zeta_t, \eta_t \}$ независимы. Авторы подчеркивают, что данное ограничение не является критически важным для построения данной модели, однако, его использование значительно снижает количество коэффициентов и упрощает интерпретацию полученных результатов. Более того, предполагается, что матрица $S$ имеет блочно-диагональную структуру, где каждый блок отвечает за параметры $\alpha_{i,t}$ одного уравнения. Так, если рассмотреть модель для трех переменных, то вектор $\alpha_t$ будет состоять из трех элементов: $(\alpha_{21,t}, \alpha_{31,t}, \alpha_{32,t})$, каждый из которых является процессом случайного блуждания \eqref{alpha}. Тогда вектор шоков имеет ковариационную матрицу $S$, имеющую следующую структуру:
\begin{align*}
S &= {\rm Var}
\begin{pmatrix}
\zeta_{21,t}\\
\zeta_{31,t}\\
\zeta_{32,t}
\end{pmatrix}
=
\begin{pmatrix}
S_1 & 0 \\
0 & S_2\\
\end{pmatrix} \quad \text{, где}\\
S_1 &= {\rm Var}(\zeta_{21,t})\\
S_2 &= 
\begin{pmatrix} 
{\rm Var}(\zeta_{31,t}) & {\rm Cov}(\zeta_{31,t}, \zeta_{32,t}) \\ 
{\rm Cov}(\zeta_{31,t}, \zeta_{32,t}) & {\rm Var}(\zeta_{32,t}) 
\end{pmatrix} 
\end{align*}

Такое ограничение подразумевает, что коэффициенты одновременных соотношений переменных в модели (вектор $\alpha_t$), изменяются независимо в каждом уравнении модели. Как и предположение о независимости шоков различных параметров между собой, данная предпосылка является некоторым упрощением, которое снижает количество вычислений, необходимых для оценки параметров. При этом авторы показывают, что данные упрощения не оказывают значительного влияния на эмпирические результаты, что свидетельствует об оправданности их использования.

Количество параметров, которые требуется оценить в модели \eqref{tvp2} -- \eqref{sigma}, обычно достигает нескольких тысяч. Так, в модели с 3 переменными и двумя лагами содержится 21 коэффициент, каждый из которых изменяется со временем (что при 100 периодах уже означает наличие 2100 коэффициентов), а число гиперпараметров (свободные элементы матриц $Q, S, W$) составляет 241 \footnote{Primiceri 2005, стр. 28}. Оценка такого количества параметров возможна только байесовским методом. Рассмотрим подробнее принцип его работы.

Байесовский подход заключается в том, что имея некоторую модель и априорные представления о ее параметрах, можно использовать формулу Байеса для условной вероятности, чтобы обновлять эти представления, используя доступные данные. Распределение, получаемое в результате, называется апостериорным. Именно его используют как источник информации о параметрах модели. Ниже представлена формула Байеса, которая показывает, как апостериорное распределение зависит от априорного и функции правдоподобия.
\begin{equation}\label{bayes1}
\Prob ( \Theta | Y_T) = \frac{\Prob (Y_T|\Theta) \Prob (\Theta)}{\Prob (Y_T)} \propto \text{Prior} \cdot \text{Likelihood}
\end{equation}

\subsubsection{Априорные распределения}
В данном разделе будут рассмотрены априорные распределения гиперпараметров модели TVP-BVAR и начальных значений процессов случайного блуждания \eqref{beta} -- \eqref{sigma}. В оригинальной статье Primiceri предполагается, что все начальные значения независимы и имеют нормальное распределение, а матрицы гиперпараметров имеют обратное распределение Уишарта и также независимы друг от друга (в том числе и блоки матрицы $S$).  Рассмотрим подробнее все априорные распределения, необходимые для оценки параметров модели.

\begin{itemize}
\item $B_0 \sim N(B_{OLS}, k_B\times\widehat{{\rm Var}}(B_{OLS}))$\\
Таким образом, начальные значения коэффициентов распределены нормально с математическим ожиданием, равным их МНК оценкам. Для получения этих значений авторы статьи, а впоследствии и автор пакета для \texttt{R}, используют первые 40 наблюдений тренировочной выборки. Значение параметра $k_B$ принимается равным 4. 

\item $A_0 \sim N(A_{OLS}, k_A \times \widehat{{\rm Var}}(A_{OLS}))$\\
Для получения математического ожидания начального значения процесса $\alpha_t$ строятся линейные регрессии остатков модели из предыдущего пункта друг на друга. То есть оцениваются регрессии следующего вида:
\begin{equation*}
\hat{u}_{i,t} = \alpha_{ij,t|OLS}\cdot \hat{u}_{j,t} +\hat{e}_t,
\end{equation*}
где $\hat{u}_{i}$ --- остаток регрессии i-ой переменной на свои лаги и лаги всех остальных переменных. Параметра $k_A$, как и в предыдущем пункте, полагается равным 4.

\item $\log (\sigma_0) \sim N(\log(\sigma_{OLS}), I_n)$\\
В данном случае МНК оценка стандартного отклонения может быть получена из остатков регрессий предыдущего пункта посредством разложения \eqref{error_decomp}.

\item $Q \sim IW(k_Q^2 \times p_Q\times \widehat{{\rm Var}}(B_{OLS}), p_Q)$\\
В оригинальной статье используются следующие параметры: $p_Q$ задается равным размеру обучающей выборке для построения МНК оценок, то есть 40, а $k_Q$ выбрано равным 0.01. 

\item $W \sim IW(k_W^2\times p_W\times I_n, p_W)$\\
Количество степеней свободы, $p_W$, выбирается в зависимости от количества переменных в модели и равно $n+1$, а $k_W = 0.01$, как и в предыдущем пункте.

\item $S_j \sim IW(k_S^2\times p_{S_j}\times \hat{{\rm Var}}(A_{j,OLS}), p_{S_j}), j = 1,...,n-1$\\
Здесь значение $k_S$ одно и то же для всех блоков матрицы $S$, равное 0.1. А вот количество степеней свободы выбирается в зависимости от размерности соответствующего блока матрицы $S$ --- $p_{S_j} = j+1$.
\end{itemize}

Заметим, что математическое ожидание случайной матрицы, имеющей обратное распределение Уишарта $IW(C,b)$, равно $\frac{C}{b-q-1}$, где $q$ - размерность матрицы $C$. И при $b \rightarrow \infty$ математическое ожидание стремится к 0. Таким образом, чем большее число степеней свободы задается в априорном распределении гиперпараметров, тем меньшую волатильность имеют шоки в процессах случайного блуждания. Если же говорить о выбранных значениях $k_Q, k_S, k_W$, то автор утверждает, что они позволяют получить неинформативное априорное распределение, гибко подстраивающееся под данные модели.

\subsubsection{Алгоритм MCMC}
К сожалению, знать априорные распределения параметров и функцию правдоподобия недостаточно для того, чтобы получить апостериорное распределение. Как видно из формулы \eqref{bayes1}, это позволит нам получить только функцию, пропорциональную апостериорному распределению. В теории не составляет труда найти значение недостающей константы по формуле полной вероятности $\Prob(Y_t) = \int \Prob(Y_T|\Theta)\Prob(\Theta)d\Theta$, но на практике очень часто посчитать такой многомерный интеграл не представляется возможным. Существует два способа решения этой проблемы: использование сопряженных распределений и применение алгоритмов MCMC для получения выборки из апостериорного распределения.

Сопряжённое априорное распределение подбирается таким образом, чтобы при вычислении его произведения с функцией правдоподобия, получалось апостериорное распределение того же семейства. В таком случае апостериорное распределение выписывается в явном виде.

Второй способ получить апостериорное распределение --- использование методов MCMC (Markov Chain Monte Carlo). Данные методы помогают получить выборку из апостериорного распределения, которая в дальнейшем может быть использована для получения интересующих нас характеристик оцениваемых параметров. Существует три основных вида алгоритмов, позволяющих получить выборку из апостериорного распределения --- сэмплирование по Гиббсу\footnote{Geman, S., \& Geman, D. (1984). Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. IEEE Transactions on pattern analysis and machine intelligence, (6), 721-741.}, алгоритм Метрополиса-Гастингса\footnote{Hastings, W. K. (1970). Monte Carlo sampling methods using Markov chains and their applications. Biometrika, 57(1), 97-109.} и гибридный метод Монте-Карло\footnote{Duane, S., Kennedy, A. D., Pendleton, B. J., \& Roweth, D. (1987). Hybrid monte carlo. Physics letters B, 195(2), 216-222.}. В основе всех этих методов лежит следующий принцип: строится цепь Маркова с заданными вероятностями перехода из одного состояния в дргугое. Стационарное состояние такой цепи и является плотностью искомого апостериорного распределения. Алгоритм инициируется в какой-то точке распределения и дальше, используя определенные критерии, начинает двигаться из одной точки в другую. При достаточно большом количестве шагов полученная выборка будет достаточно близка к выборке из искомого апостериорного распределения.

Для получения выборки из апостериорного распределения в модели \EN{TVP-BVAR} используется схема Гиббса. Рассмотрим алгоритм из статьи Primiceri, Del Negro (2015). Для простоты обозначим $B^T$ все коэффициенты $\{B_t\}_{t=1}^{T}$, аналогичные обозначения введем для $\Omega^T$ и $A^T$ и $y^T$. А все ковариационные матрицы шоков объединим в матрицу $V$. В таком случае, алгоритм формулируется следующим образом:
\begin{description}
\item[Шаг 1] Инициализируем алгоритм, задав значения параметров $A^T, \Omega^T, V и s^T$;
\item[Шаг 2] Генерируем значения $B^T$ из распределения $\Prob(B^T|A^T, \Omega^t, V, y^T)$;
\item[Шаг 3] Генерируем элементы $Q$ из распределения $\Prob(Q|A^T, B^T, \Omega^T, y^T)$, являющегося обратным распределением Уишарта;
\item[Шаг 4] Генерируем элементы $A^T$ из распределения $\Prob (A^T|B^T, \Omega^T, V, y^T)$;
\item[Шаг 5] Генерируем элементы блоков $S$ из распределения $\Prob(S|A^T, B^T, \Omega^T, y^T)$, являющегося обратным распределением Уишарта;
\item[Шаг 6] Генерируем вспомогательную дискретную переменную $s^T$ из распределения $\Prob(s^T|A^T, B^T, \Omega^T, V, y^T)$;
\item[Шаг 7] Генерируем элементы $\Omega^T$ из распределения $\Prob(\Omega^T|A^T, B^T, V, y^T, s^T)$; 
\item[Шаг 8] Генерируем элементы $W$ из распределения $\Prob(W|A^T, B^T, \Omega^T, y^T)$, являющегося обратным распределением Уишарта;
\item[Шаг 8] Возвращаемся к пункту 2.
\end{description}
 
На шаге 2, 4 и 7 для получения реализации случайных величин $B^T, A^T$ и $\Omega^T$ используется алгоритм из статьи Carter, Kohn (1994), в которой было показано как можно сгенерировать параметры в линейных моделях пространства состояний, используя фильтр Калмана. 

Шаги 3, 5 и 8 представляют собой стандартную процедуру получения реализации случайной величины, имеющей обратное распределение Уишарта.

Применить алгоритм Carter, Kohn на шаге 7 напрямую, как это делается на 2 и 4 шаге, невозможно, так как если мы представим модель в линейной форме, как того требует данный алгоритм, то шоки в ней будут иметь не нормальное распределение, предполагаемое в модели, а представляют собой логарифм распределения хи-квадрат. Для решения этой проблемы используется алгоритм из работы Kim, Shephard and Chib (1998), где логарифм хи-квадрат распределения аппроксимируется набором из 7 нормальных распределений. Вспомогательная переменная $s^T$ определяет, какой состав нормальных распределений будет использоваться в каждый момент времени для аппроксимации.
\subsubsection{Прогнозирование}
Если основной целью является построение прогнозов, то помимо апостериорного распределения параметров необходимо получить апостериорное распределение будущих значений зависимых переменных при условии всех доступных данных:
\begin{align}\label{pred-distr}
\Prob (Y_{T+1:T+H}|Y_T) &= \frac{\Prob(Y_{T+1:T+H},Y_T)}{\Prob(Y_T)} = \\
 &= \frac{\int \Prob(Y_{T+1:T+H}|Y_T, \Theta)\cdot\Prob(Y_T|\Theta)\cdot\Prob(\Theta)d\Theta}{\int \Prob(Y_T|\Theta)\cdot\Prob(\Theta)d\Theta} \nonumber
\end{align}
Получение предиктивного распределения на практике осуществляется не напрямую с использованием выражения  \eqref{pred-distr},  а через апостериорное распределение параметров.\\
\begin{align*}
\Prob (Y_{T+1:T+H}|Y_T) &= \int \Prob(Y_{T+1:T+H}|Y_T, \Theta)\cdot \Prob(\Theta|Y_T d\Theta =\\
&=\int \prod_{t = T+1}^{T+H}\Prob(Y_{t}|Y_T, \Theta)\cdot \Prob(\Theta|Y_T) d\Theta
\end{align*}
Как и в случае с апостериорным распределением посчитать предиктивное распределение аналитически часто бывает невозможно, поэтому для получения прогнозов также используются MCMC алгоритмы. Рассмотрим алгоритм для получения выборки из предиктивного распределения, использующийся в пакете \texttt{bvarsv}.

\begin{description}
\item[Шаг 1] Генерируем параметры из апостериорного распределения на $H$ периодов вперед;
\item[Шаг 2] Генерируем вектор ошибок $\varepsilon_{T+j}, j = 1,...,H$ из стандартного нормального распределения;
\item[Шаг 3] Используя значения, полученные на предыдущих шагах, генерируем будущие значения переменных, используя уравнение \eqref{tvp1};
\item[Шаг 4] Для каждого $j = 1,…,H$ считаем математическое ожидание и ковариационную матрицу будущего значения $Y_{T+j}$.
\end{description}

Чтобы понять, что именно считать прогнозом модели (это может быть, например, математическое ожидание предиктивного распределения, его мода или медиана), нужно определить функцию потерь, которая зависит от будущих значений переменной и наших прогнозов. Определившись с функцией потерь, мы можем найти такую функцию для определения прогноза, при которой значение функции потерь минимально. Чаще всего используется функция потерь, представляющая собой квадрат отклонений прогноза от реального значения переменной $Y$. В таком случае оптимальной функцией для определения прогноза является условное математическое ожидание ${\rm E} (Y_{T+1:T+H}|Y_T)$. В данной работе в качестве точечного прогноза также используется математическое ожидание.
